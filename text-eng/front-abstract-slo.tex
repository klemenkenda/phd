%--------------------------------------------------------------------------------------------------
% 
\chapter*{Povzetek}
\pdfbookmark[0]{Povzetek}{Povzetek}
%--------------------------------------------------------------------------------------------------

S hitrim razvojem senzorskih tehnologij, še posebej v okviru interneta stvari (IoT), smo vstopili v obdobje, ki ga zaznamujejo velike količine podatkov, ki so na voljo v realnem času. 
S tem so nastale potrebe po novih metodah obdelave podatkov, ki omogočajo prehod od tradicionalne paketne (batch) analize do uporabe metod analize podatkov v realnem času.

Doktorska disertacija se ukvarja s prenosom analitičnih rešitev iz nadzorovanega laboratorijskega v realno okolje.
Posebej se ukvarjamo s problemi avtonomnega čiščenja, obogatitve in združevanje podatkov v realnem času, izbire značilk ter izdelave ustreznih informacijskih rešitev.
Jedro te študije predstavlja nova tehnika inkrementalnega združevanja podatkov iz heterogenih podatkovnih tokov v vektorje vektorjev značilk, primerne za uporabo v modelih strojnega učenja.
Pomembnost takšne metodologije je bila v večini študij na tem področju spregledana, saj se slednje večinoma osredotočajo zgolj na učinkovitost modelov strojnega učenja.
Te študije predpostavljajo, da so uporabljeni podatki pravilni, časovno usklajeni in vedno dostopni, kar v realnih scenarijih redko drži.

Cilj te naloge je razviti arhitekturo, ki presega zgoraj navedene privzetke. 
V disertaciji najprej predstavimo metodologijo čiščenja podatkov, ki izkorišča zmožnost Kalmanovega filtra za izdelavo kratkoročnih napovedi, vključno z napovedovanjem variance. 
Ta metoda se lahko uporablja za čiščenje podatkovnih tokov, katerih vzorčenje je veliko višje od hitrosti sprememb merjenih pojavov, kar je običajno pri internetu stvari (IoT). 
Nadalje predstavimo metodologijo za sprotno združevanje množice heterogenih virov pretočnih podatkov v vektorje značilk. 
Predlagana metodologija zmore preseči izzive, ki nastanejo zaradi heterogenih podatkovnih tokov, vključno s časovnim odstopanjem posameznih meritev in različno hitrostjo vzorčenja meritev. 
Poleg tega sistem omogoča tudi vključevanje napovedi in predhodno izračunanih vrednosti v vektorje značilk. 
S pomočjo opisane metodologije je sistem sposoben generiranja vektorjev značilk, ki vključujejo časovno usklajene podatke iz različnih virov, agregirane in obogatene vrednosti, odložene vrednosti, statične vrednosti, in vrednosti relevantnih napovedi, kot so npr. vremenske napovedi. 
Takšen sistem je sposoben ustvarjanja zelo obsežnih vektorjev značilk, ki omogočajo učinkovito modeliranje.
Velika količina značilk pa ne vodi nujno do najbolj optimalnih modelskih rezultatov, zato v nadaljevanju disertacije predstavimo še algoritem za izbiro značilk FASTENER, ki uporablja genetske algoritme in večkriterijsko optimizacijo. 
Algoritem je bil posebej zasnovan za nalogo segmentacije satelitskih slik za potrebe v kmetijstvu, vendar pa je pokazal nepričakovano učinkovitost tudi v različnih drugih primerih, npr. za napovedovanje časovnih vrst v energetiki, prometu in upravljanju z vodo. 
Vse predstavljene rešitve na koncu postavimo v lambda arhitekturo v okviru masovnih podatkov (Big Data).
Lambda arhitekturi tako dodamo analitične zmožnosti, ki niso omejene zgolj na zaznavanje dogodkov.
Predlagano arhitekturo smo uporabili pri implementaciji sistema za podporo odločanju na področju upravljanja voda. 
Na istem področju smo preizkusili tudi uporabnost algoritmov za inkrementalno učenje, kot so npr. Hoeffdingova drevesa, in jih primerjali s tradicionalnimi paketnimi metodami.

Učinkovitost predlaganega pristopa smo ocenili v več scenarijih, ki obsegajo področja, kot so upravljanje z energijo, prometom in okoljem. 
Najnovejša implementacija celotne rešitve je bila izvedena v okviru evropskega projekta H2020 NAIADES, kar dokazuje njeno uporabnost na področju upravljanja voda.